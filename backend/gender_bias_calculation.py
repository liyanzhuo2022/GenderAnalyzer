# -*- coding: utf-8 -*-
"""gender bias calculation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EuIFu-QBpy4PGQ6qGd2anIGqaL_VGuC8
"""

import re
import nltk
import textwrap
from nltk.corpus import wordnet
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

nltk.download('wordnet')

# Gender-coded word lists
masculine_coded = ['advanced', 'advantageous', 'agile', 'agreed', 'analytical', 'applicable', 'assigned', 'attractive',
                   'automotive', 'away', 'azure', 'base', 'broad', 'built', 'capable', 'chance', 'changing', 'choice',
                   'civil', 'comfortable', 'completing', 'concrete', 'confident', 'connected', 'consistent', 'continued',
                   'contributing', 'coordinate', 'coordinating', 'correct', 'critical', 'cutting', 'designing', 'desired',
                   'detailed', 'developed', 'domestic', 'due', 'east', 'electrical', 'environmental', 'executive',
                   'existing', 'expert', 'express', 'fixed', 'fleet', 'functional', 'happy', 'heavy', 'hourly', 'hybrid',
                   'improving', 'incident', 'incoming', 'industrial', 'initial', 'integrated', 'interstate', 'involved',
                   'labouring', 'latest', 'listed', 'maintained', 'major', 'material', 'mechanical', 'medium', 'metal',
                   'mobile', 'model', 'moving', 'much', 'needed', 'northern', 'outside', 'overall', 'pass', 'port',
                   'practical', 'prepared', 'punctual', 'reasonable', 'received', 'recent', 'remote', 'removed', 'rigid',
                   'same', 'secure', 'self', 'significant', 'skilled', 'solid', 'stable', 'starting', 'sustainable',
                   'talented', 'thinking', 'trusted', 'used', 'varied', 'waste', 'west']

feminine_coded = ['academic', 'aged', 'allied', 'approved', 'beautiful', 'caring', 'catholic', 'clinical', 'creative',
                  'educational', 'everyday', 'fresh', 'front', 'meaningful', 'mental', 'patient', 'perfect',
                  'proof', 'registered', 'resident', 'special', 'temporary', 'vaccinated', 'vibrant', 'welcoming', 'young']

all_bias_words = set(masculine_coded + feminine_coded)

# Load paraphrasing model
tokenizer = AutoTokenizer.from_pretrained("Vamsi/T5_Paraphrase_Paws")
model = AutoModelForSeq2SeqLM.from_pretrained("Vamsi/T5_Paraphrase_Paws")
paraphraser = pipeline("text2text-generation", model=model, tokenizer=tokenizer)

def get_neutral_synonym(word):
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            candidate = lemma.name().replace("_", " ").lower()
            if candidate != word and candidate not in all_bias_words:
                return candidate
    return word

def underline_gender_words(text, masc_words, fem_words):
    gender_words = set(masc_words + fem_words)
    pattern = re.compile(r'\b(' + '|'.join(map(re.escape, gender_words)) + r')\b', re.IGNORECASE)

    def replacer(match):
        return f"<u>{match.group(0)}</u>"

    return pattern.sub(replacer, text)

def remove_gender_bias(text):
    def replace(match):
        word = match.group(0)
        replacement = get_neutral_synonym(word.lower())
        if word.istitle():
            replacement = replacement.title()
        elif word.isupper():
            replacement = replacement.upper()
        return replacement

    pattern = re.compile(r'\b(' + '|'.join(re.escape(w) for w in all_bias_words) + r')\b', re.IGNORECASE)
    return pattern.sub(replace, text)

def analyze_gender_bias(text):
    return analyze_gender_bias_helper(text, masculine_coded, feminine_coded)

def analyze_gender_bias_helper(text, masc_list, fem_list):
    text_lower = text.lower()
    total_words = len(re.findall(r'\b\w+\b', text_lower))
    masc_hits = [w for w in masc_list if re.search(rf'\b{re.escape(w)}\b', text_lower)]
    fem_hits = [w for w in fem_list if re.search(rf'\b{re.escape(w)}\b', text_lower)]

    masc_count = len(masc_hits)
    fem_count = len(fem_hits)

    if masc_count + fem_count == 0:
        friendliness_score = 10.0
        bias_category = "Balanced"
    else:
        balance_ratio = min(masc_count, fem_count) / max(masc_count, fem_count)
        friendliness_score = round(balance_ratio * 10, 1)
        if friendliness_score > 8.0:
            bias_category = "Balanced"
        elif masc_count > fem_count:
            bias_category = "Agentic"
        else:
            bias_category = "Communal"

    # Rewritten posting (underlined)
    rewritten = underline_gender_words(text, masc_hits, fem_hits)

    # Step 1: Remove gender-coded words via synonym replacement
    neutral_text = remove_gender_bias(text)

    # Step 2: Generate improved neutral text via paraphrasing model
    prompt = f"paraphrase: {neutral_text} </s>"
    output = paraphraser(prompt, max_length=512, num_beams=4, num_return_sequences=1)[0]['generated_text']

    # Format to match original layout
    formatted_output = format_to_match(text, output)

    return {
        "friendliness_score": friendliness_score,
        "dimension": bias_category,
        "gender_word_distribution": {
            "male-coded": masc_count,
            "female-coded": fem_count
        },
        "detected_gender_words": {
            "male-coded": masc_hits,
            "female-coded": fem_hits
        },
        # "rewritten_posting": rewritten,
        "rewritten_posting": formatted_output
    }

def format_to_match(original_text, regenerated_text):
    # Simple paragraph matching (could be made more robust)
    orig_lines = original_text.strip().splitlines()
    regen_sentences = re.split(r'(?<=[.?!])\s+', regenerated_text.strip())

    formatted = ""
    i = 0
    for line in orig_lines:
        if line.strip() == "":
            formatted += "\n"
        elif line.strip().endswith(":"):
            formatted += line + "\n"
        elif line.strip().startswith("-"):
            if i < len(regen_sentences):
                formatted += "- " + regen_sentences[i].strip() + "\n"
                i += 1
        else:
            if i < len(regen_sentences):
                formatted += regen_sentences[i].strip() + "  \n"
                i += 1

    return formatted

def print_analysis_result(result):
    print("Friendliness Score:", result['friendliness_score'])
    print("  - This score ranges from 0 to 10, where 10 means perfectly balanced use of masculine and feminine coded words.")
    print("  - It is calculated as: (min(masc_count, fem_count) / max(masc_count, fem_count)) * 10")
    print("  - If no gender-coded words are found, it defaults to 10 (perfect balance).\n")

    print("Bias Dimension:", result['dimension'])
    print("  - 'Agentic' means more masculine-coded words.")
    print("  - 'Communal' means more feminine-coded words.")
    print("  - 'Balanced' means neither side dominates significantly.\n")

    print("Gender Word Distribution:")
    print(f"  - Male-coded words count: {result['gender_word_distribution']['male-coded']}")
    print(f"  - Female-coded words count: {result['gender_word_distribution']['female-coded']}\n")

    print("Detected Gender-coded Words:")
    print("  - Male-coded words found:", result['detected_gender_words']['male-coded'])
    print("  - Female-coded words found:", result['detected_gender_words']['female-coded'], "\n")

    print("Rewritten Posting (gender-coded words underlined):")
    print(result['rewritten_posting'])

    print("\n--- Regenerated Posting (neutral) ---\n")
    print(result['regenerated_posting'])
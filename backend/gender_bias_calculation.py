
# -*- coding: utf-8 -*-
"""gender bias calculation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EuIFu-QBpy4PGQ6qGd2anIGqaL_VGuC8
"""

import re
import json

masculine_coded_words = [
    "active", "adventurous", "aggress", "ambitio", "analy", "assert", "athlet", "autonom",
    "battle", "boast", "challeng", "champion", "compet", "confident", "courag", "decid",
    "decision", "decisive", "defend", "determin", "domina", "dominant", "driven", "fearless",
    "fight", "force", "greedy", "head-strong", "headstrong", "hierarch", "hostil", "impulsive",
    "independen", "individual", "intellect", "lead", "logic", "objective", "opinion", "outspoken",
    "persist", "principle", "reckless", "self-confiden", "self-relian", "self-sufficien",
    "selfconfiden", "selfrelian", "selfsufficien", "stubborn", "superior", "unreasonab"
]

feminine_coded_words = [
    "agree", "affectionate", "child", "cheer", "collab", "commit", "communal", "compassion",
    "connect", "considerate", "cooperat", "co-operat", "depend", "emotiona", "empath", "feel",
    "flatterable", "gentle", "honest", "interpersonal", "interdependen", "interpersona",
    "inter-personal", "inter-dependen", "inter-persona", "kind", "kinship", "loyal", "modesty",
    "nag", "nurtur", "pleasant", "polite", "quiet", "respon", "sensitiv", "submissive", "support",
    "sympath", "tender", "together", "trust", "understand", "warm", "whin", "enthusias",
    "inclusive", "yield", "share", "sharin"
]

def analyze_gender_bias(text):
    # Convert text to lowercase
    text_lower = text.lower()

    # Count total number of words
    total_words = len(re.findall(r'\b\w+\b', text_lower))

    # Lists to collect matched masculine/feminine words
    masc_hits = []
    fem_hits = []

    # Match masculine-coded stems using regular expressions
    for stem in masculine_coded_words:
        matches = re.findall(rf'\b({stem}\w*)\b', text_lower)
        masc_hits.extend(matches)

    # Match feminine-coded stems using regular expressions
    for stem in feminine_coded_words:
        matches = re.findall(rf'\b({stem}\w*)\b', text_lower)
        fem_hits.extend(matches)

    # Count the number of masculine and feminine words found
    masc_count = len(masc_hits)
    fem_count = len(fem_hits)

    # Calculate gender-friendliness score (0-10)
    if masc_count + fem_count == 0:
        gender_friendliness = 10.0  # Perfect score if no gendered words
    else:
        balance_ratio = min(masc_count, fem_count) / max(masc_count, fem_count) if max(masc_count, fem_count) > 0 else 1
        gender_friendliness = round(balance_ratio * 10, 1)

    # Determine bias category
    if masc_count > fem_count * 1.5:  # Threshold for agentic
        bias_category = "Agentic"
    elif fem_count > masc_count * 1.5:  # Threshold for communal
        bias_category = "Communal"
    else:
        bias_category = "Balanced"

    # Generate rewritten version (simple implementation)
    rewritten_text = text  # Placeholder - actual rewriting would be more complex

    # Return analysis results in requested format
    return {
        "friendliness_score": gender_friendliness,
        "dimension": bias_category,
        "gender_word_distribution": {
            "male-coded": masc_count,
            "female-coded": fem_count
        },
        "detected_gender_words": {
            "male-coded": list(set(masc_hits)),
            "female-coded": list(set(fem_hits))
        },
        "additional_metrics": {
            "total_words": total_words,
            "bias_word_percentage": round((masc_count + fem_count) / total_words * 100, 2) if total_words > 0 else 0
        },
        "rewritten_posting": rewritten_text
    }

# # Example usage
# job_text = """
# We're looking for a driven and competitive sales leader who is confident, decisive, and fearless in closing deals.
# You'll work independently and challenge yourself to exceed goals in a dynamic environment.
# Our team values collaboration, understanding client needs, and building community.
# """

# result = analyze_gender_bias(job_text)
# print(json.dumps(result, indent=2))
